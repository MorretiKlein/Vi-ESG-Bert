{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from trl import SFTTrainer\n",
    "import os,torch, wandb, platform, gradio, warnings\n",
    "from pyvi import ViTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:45:07.491364Z",
     "start_time": "2022-06-02T02:45:07.482746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.7.0+cu118', '4.51.3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:18.286115Z",
     "start_time": "2022-06-02T02:44:18.222850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:06.431016Z",
     "start_time": "2022-06-02T02:54:06.391735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th√™m d·∫•u ph·∫©y sau tr√™n 700 ngh√¨n t·ª∑ ƒë·ªìng v·ªën ƒë...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L·ª±a ch·ªçn c·ªï phi·∫øu ni√™m y·∫øt c√≥ ch·ªâ s·ªë PB cao ƒë∆∞...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ch√≠nh s√°ch b·∫£o hi·ªÉm n√¥ng nghi·ªáp l√† tia hy v·ªçng...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ngo√†i ra Vingroup cam k·∫øt ph√°t tri·ªÉn c√°c d·ª± √°n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Di·ªán t√≠ch tr·ªìng c√† gai leo t·∫°i ƒë·ªãa ph∆∞∆°ng l√† k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37631</th>\n",
       "      <td>Ch√∫ng t√¥i l√† m·ªôt trong nh·ªØng c√¥ng ty Th·ª•y Sƒ© ƒë...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37632</th>\n",
       "      <td>Ch√∫ng ta s·∫Ω ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u n√†y b·∫±ng c√°ch tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37633</th>\n",
       "      <td>ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c·∫£i thi·ªán hi·ªáu qu·∫£ ƒëi·ªán n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37634</th>\n",
       "      <td>NƒÉm 2021 ch√∫ng t√¥i ƒë√£ ti·∫øn h√†nh b∆∞·ªõc ƒë·∫ßu ti√™n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37635</th>\n",
       "      <td>M·ª•c ƒë√≠ch l√† ƒë·ªÉ khai th√°c ti·ªÅm nƒÉng c·ªßa ch√≠nh c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37636 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentences  labels\n",
       "0      Th√™m d·∫•u ph·∫©y sau tr√™n 700 ngh√¨n t·ª∑ ƒë·ªìng v·ªën ƒë...       0\n",
       "1      L·ª±a ch·ªçn c·ªï phi·∫øu ni√™m y·∫øt c√≥ ch·ªâ s·ªë PB cao ƒë∆∞...       0\n",
       "2      Ch√≠nh s√°ch b·∫£o hi·ªÉm n√¥ng nghi·ªáp l√† tia hy v·ªçng...       2\n",
       "3      Ngo√†i ra Vingroup cam k·∫øt ph√°t tri·ªÉn c√°c d·ª± √°n...       2\n",
       "4      Di·ªán t√≠ch tr·ªìng c√† gai leo t·∫°i ƒë·ªãa ph∆∞∆°ng l√† k...       1\n",
       "...                                                  ...     ...\n",
       "37631  Ch√∫ng t√¥i l√† m·ªôt trong nh·ªØng c√¥ng ty Th·ª•y Sƒ© ƒë...       1\n",
       "37632  Ch√∫ng ta s·∫Ω ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u n√†y b·∫±ng c√°ch tr...       1\n",
       "37633  ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c·∫£i thi·ªán hi·ªáu qu·∫£ ƒëi·ªán n...       1\n",
       "37634  NƒÉm 2021 ch√∫ng t√¥i ƒë√£ ti·∫øn h√†nh b∆∞·ªõc ƒë·∫ßu ti√™n ...       1\n",
       "37635  M·ª•c ƒë√≠ch l√† ƒë·ªÉ khai th√°c ti·ªÅm nƒÉng c·ªßa ch√≠nh c...       1\n",
       "\n",
       "[37636 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_train = \"data_pseudo_vireport/train_data_high.csv\"\n",
    "filename_test = \"data_pseudo_vireport/final_test.csv\"\n",
    "\n",
    "data_train = pd.read_csv(filename_train)\n",
    "data_test = pd.read_csv(filename_test)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    14155\n",
       "0     9511\n",
       "3     8203\n",
       "1     5767\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    996\n",
       "3    924\n",
       "1    570\n",
       "0    510\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load FinBERT pretrained model\n",
    "The pretrained FinBERT model path on Huggingface is https://huggingface.co/yiyanghkust/finbert-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:17.945203Z",
     "start_time": "2022-06-02T02:54:10.422200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at NlpHUST/vi-electra-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(62000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = ElectraTokenizer.from_pretrained('NlpHUST/vi-electra-small')\n",
    "model = ElectraForSequenceClassification.from_pretrained('NlpHUST/vi-electra-small',num_labels=4)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Keep only the classification head trainable\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:37:33.687054Z",
     "start_time": "2022-06-02T02:37:33.664650Z"
    }
   },
   "source": [
    "### prepare dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:33.660010Z",
     "start_time": "2022-06-02T02:54:17.948143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f39f5b497445a2bdd54ae5b91e37c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3573c090c47e4406a93a332f0babcdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = simple_preprocess(text)\n",
    "    # Gh√©p l·∫°i th√†nh c√¢u\n",
    "    text = ' '.join(tokens)\n",
    "    # T√°ch t·ª´ ki·ªÉu ti·∫øng Vi·ªát (gi·ªØ ƒë·ªãnh d·∫°ng t·ª´ gh√©p c√≥ d·∫•u g·∫°ch d∆∞·ªõi)\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    return text\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω c·ªôt vƒÉn b·∫£n trong t·ª´ng DataFrame\n",
    "data_train['Sentences'] = data_train['Sentences'].apply(preprocess_text)\n",
    "data_test['Sentences'] = data_test['Sentences'].apply(preprocess_text)\n",
    "\n",
    "# Chuy·ªÉn sang Dataset c·ªßa HuggingFace\n",
    "dataset_train = Dataset.from_pandas(data_train)\n",
    "dataset_val = Dataset.from_pandas(data_train)\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(lambda e: tokenizer(e['Sentences'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: tokenizer(e['Sentences'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Sentences', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 37636\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([32, 128])\n",
      "Labels shape: torch.Size([32])\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=32, \n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc batch\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "    print((batch[\"labels\"][0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pc/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrung235689\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pc/Code/ESG code/VietNam FinBert/wandb/run-20250612_165902-2gos5yzk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trung235689/viELECTRA/runs/2gos5yzk?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af' target=\"_blank\">mild-snowflake-2</a></strong> to <a href='https://wandb.ai/trung235689/viELECTRA?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trung235689/viELECTRA?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af' target=\"_blank\">https://wandb.ai/trung235689/viELECTRA?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trung235689/viELECTRA/runs/2gos5yzk?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af' target=\"_blank\">https://wandb.ai/trung235689/viELECTRA/runs/2gos5yzk?apiKey=e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=\"e7056d3a3bc855a6e0d38b8c4ff7d2ec7ce895af\")\n",
    "run = wandb.init(project='viELECTRA', job_type=\"training\", anonymous=\"allow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(model.config.num_labels)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:57:15.963784Z",
     "start_time": "2022-06-02T02:54:33.662575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11770' max='11770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11770/11770 23:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.223027</td>\n",
       "      <td>0.934956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.953157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.960517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.159131</td>\n",
       "      <td>0.949968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.068444</td>\n",
       "      <td>0.980710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>0.971782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.084879</td>\n",
       "      <td>0.972712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.048593</td>\n",
       "      <td>0.986795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.985891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.050178</td>\n",
       "      <td>0.985785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11770, training_loss=0.16118919288291705, metrics={'train_runtime': 1420.2293, 'train_samples_per_second': 264.999, 'train_steps_per_second': 8.287, 'total_flos': 2768242574745600.0, 'train_loss': 0.16118919288291705, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy' : accuracy_score(predictions, labels)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = 'temp/',\n",
    "        eval_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        learning_rate= 2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=10,  # 1 epoch\n",
    "        weight_decay=0.005,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "        args=args,                  # training arguments, defined above\n",
    "        train_dataset=dataset_train,         # training dataset\n",
    "        eval_dataset=dataset_val,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### evaluate on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T03:09:08.434635Z",
     "start_time": "2022-06-02T03:09:06.980671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.04859252646565437,\n",
       " 'test_accuracy': 0.9867945584015304,\n",
       " 'test_runtime': 32.0251,\n",
       " 'test_samples_per_second': 1175.205,\n",
       " 'test_steps_per_second': 36.752}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.predict(dataset_val).metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T03:09:11.599174Z",
     "start_time": "2022-06-02T03:09:10.847115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./VIELECTRA_train/vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./VIELECTRA_train/tokenizer_config.json',\n",
       " './VIELECTRA_train/special_tokens_map.json',\n",
       " './VIELECTRA_train/vocab.txt',\n",
       " './VIELECTRA_train/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('VIELECTRA_train/')\n",
    "tokenizer.save_pretrained(\"./VIELECTRA_train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class  Accuracy\n",
      "0     0  0.977605\n",
      "1     1  0.994625\n",
      "2     2  0.992441\n",
      "3     3  0.982202\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = trainer.predict(dataset_val)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# --- 9. H√†m t√≠nh accuracy theo t·ª´ng l·ªõp ---\n",
    "def save_per_class_accuracy(y_true, y_pred, class_names=None, output_path=\"viELECTRA_per_class_accuracy.csv\"):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    acc_per_class = []\n",
    "\n",
    "    for label in labels:\n",
    "        idx = y_true == label\n",
    "        acc = accuracy_score(y_true[idx], y_pred[idx])\n",
    "        class_label = class_names[label] if class_names else f\"Class {label}\"\n",
    "        acc_per_class.append((class_label, acc))\n",
    "\n",
    "    df = pd.DataFrame(acc_per_class, columns=[\"Class\", \"Accuracy\"])\n",
    "    print(df)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# --- 10. G·ªçi h√†m ---\n",
    "class_names = [\"0\", \"1\", \"2\", \"3\"]  # b·∫°n c√≥ th·ªÉ thay b·∫±ng t√™n th·∫≠t c·ªßa t·ª´ng l·ªõp n·∫øu c√≥\n",
    "save_per_class_accuracy(labels, preds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9867945584015304"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "overall_accuracy = accuracy_score(labels, preds)\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
